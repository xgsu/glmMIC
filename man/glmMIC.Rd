% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/glmMIC.R
\name{glmMIC}
\alias{glm.MIC}
\alias{glmMIC}
\title{Sparse Estimation of a GLM via Minimum approximated Information Criterion}
\usage{
glmMIC(formula, preselection = NULL, family = c("gaussian", "binomial",
  "poisson"), data, beta0 = NULL, preselect.intercept = FALSE,
  criterion = "BIC", lambda0 = 0, a0 = NULL, rounding.digits = 4,
  use.GenSA = FALSE, lower = NULL, upper = NULL, maxit.global = 100,
  maxit.local = 100, epsilon = 1e-06, se.gamma = TRUE, CI.gamma = TRUE,
  conf.level = 0.95, se.beta = TRUE, fit.ML = FALSE, details = FALSE)
}
\arguments{
\item{formula}{An object of class \code{\link[stats]{formula}}, with the response on the left of a \code{~} operator, and the terms on the right.}

\item{preselection}{A formula of form, e.g., \code{~ x1 + x2} that gives the pre-selected variables. In this case, no penalty will be applied to their slope parameters. These variables must be contained in the \code{formula} argument; otherwise, an error message shows up.}

\item{family}{a description of the error distribution and link function to be used in the model. Preferably for computational speed,
this is a character string naming a family function among the following three choices: \code{"gaussian"},
\code{"binomial"}, or \code{"poisson"}.  Otherwise, it has to be a family function or the result of a call to a family function that
can be called for by \code{\link[stats]{glm.fit}}. See \code{\link[stats]{family}} for details of family functions.}

\item{data}{A data.frame in which to interpret the variables named in the \code{formula} argument.}

\item{beta0}{User-supplied beta0 value, the starting point for optimization. If missing or \code{NULL} (by default), the maximum likelihood estimator (MLE) will be used.}

\item{preselect.intercept}{A logical value indicating whether the intercept term is pre-selected. By default, it is \code{FALSE}.}

\item{criterion}{Specifies the model selection criterion used. If \code{"AIC"}, the complexity penalty parameter (lambda)
equals 2; if \code{"BIC"}, lambda equals ln(n), where n is the sample size. You may specify
the penalty parameter of your choice by setting \code{lambda0}.}

\item{lambda0}{User-supplied penalty parameter for model complexity. If \code{method="AIC"} or \code{"BIC"}, the value
of \code{lambda0} will be ignored.}

\item{a0}{The scale (or sharpness) parameter used in the hyperbolic tangent penalty. By default, \code{a0=min(n, 100)} is used.}

\item{rounding.digits}{Number of digits after the decimal point for rounding-up estiamtes. Default value is 4.}

\item{use.GenSA}{Logical value indicating if the generalized simulated annealing \code{GenSA} is used. The default is \code{FALSE}.}

\item{lower}{The lower bounds for the search space in \code{GenSA}. The default is -10 (\eqn{p} by \eqn{1} vector).}

\item{upper}{The upper bounds for the search space in \code{GenSA}. The default is +10 (\eqn{p} by \eqn{1} vector).}

\item{maxit.global}{Maximum number of iterations allowed for the global optimization algorithm \code{SANN}. Default value is 100.}

\item{maxit.local}{Maximum number of iterations allowed for the local optimizaiton algorithm \code{BFGS}. Default value is 100.}

\item{epsilon}{Tolerance level for convergence. Default is 1e-6.}

\item{se.gamma}{Logical indicator of whether the standard error for \code{gamma} is computed. Default is \code{TRUE}.}

\item{CI.gamma}{Logical indicator of whether the confidence inverval for \code{gamma} is outputed. Default is \code{TRUE}.}

\item{conf.level}{Specifies the confidence level for \code{CI.gamma}. Defaulted as 0.95.}

\item{se.beta}{Logical indicator of whether the (post-selection) standard error for \code{beta} is computed. Default is \code{TRUE}.}

\item{fit.ML}{Logical indicator of whether we fit the best selected model with full iteration of maximum likelihood (ML). Default is \code{FALSE}.}

\item{details}{Logical value: if \code{TRUE}, detailed results will be printed out when running \code{coxphMIC}.}
}
\value{
An object of class \code{glmMIC} is returned, which may contain the following components depending on the options.
\describe{
\item{opt.global}{Results from the preliminary run of a global optimization procedure (\code{SANN} as default).}
\item{opt.local}{Results from the second run of a local optimization procedure (\code{BFGS} as default).}
\item{min.Q}{Value of the minimized objective function.}
\item{gamma}{Estimated gamma (reparameterized);}
\item{beta}{Estimated beta;}
\item{VCOV.gamma}{The estimated variance-covariance matrix for the (reparameterized) gamma estimate;}
\item{se.gamma}{Standard errors for the gamma estimate;}
\item{VCOV.beta}{The estimated variance-covariance matrix for the beat estimate;}
\item{se.beta}{Standard errors for the beta estimate (post-selection);}
\item{BIC}{The BIC value for the \emph{selected} model;}
\item{result}{A summary table of the fitting results;}
\item{fit.ML}{The \code{glm} fitting results with the selected model with full ML iterations;}
\item{call}{the matched call.}
}
}
\description{
Sparse Estimation of a GLM via Minimum approximated Information Criterion
}
\details{
The main idea of MIC involves approximation of the l0 norm with a continuous or smooth
unit dent function. This method bridges the best subset selection and regularization by
borrowing strength from both. It mimics the best subset selection using a penalized likelihood
approach yet with no need of a tuning parameter.

The problem is further reformulated with a reparameterization step by relating \code{beta}
to \code{gamma}. There are two benefits of doing so:  first, it reduces the optimization to
one unconstrained nonconvex yet smooth programming problem, which can be solved efficiently
as in computing the maximum likelihood estimator (MLE); furthermore, the
reparameterization tactic yields an additional advantage in terms of circumventing post-selection inference.
Significance testing on \code{beta} can be done through \code{gamma}.

To solve the smooth yet nonconvex optimization, two options are available. The first is a simulated annealing (\code{method="SANN"} option
in \code{\link[stats]{optim}}) global optimization algorithm is first applied. The resultant estimator is then used
as the starting point for another local optimization algorithm, where the quasi-Newton BFGS method (\code{method="BFGS"}
in \code{\link{optim}}) by default. Optionally, the generalized simulated annealing, implemented in \code{\link[GenSA]{GenSA}},
can be used instead. This latter approach tends to be slower. However, it does not need to be combined with another local optimization;
besides, it often yields the same final solution with different runs. Thus, when \code{use.GenSA=TRUE},
the output includes \code{opt.global} only, without \code{opt.local}.

In its current version, some appropriate data preparation might be needed. Most important of all,  X variables in all scenarios need to be
standardized or scaled. In the case of Gaussian linear regression, the response variable needs to be centered or even standardized.
In addition, missing values would cause errors too and hence need prehanlding too.
}
\examples{
  # Note that glmMIC works with standardized data only. See below for examples.
  # GAUSSIAN LINEAR REGRESSION
  library(lars); data(diabetes);
  dat <- cbind(diabetes$x, y=diabetes$y)
  dat <- as.data.frame(scale(dat))
  fit.MIC <- glmMIC(formula=y~.-1, family="gaussian", data=dat)
  names(fit.MIC)
  print(fit.MIC)
  plot(fit.MIC)
  # WITH PRE-SELECTED VARIABLES AND A DIFFERENT a VALUE
  fit.MIC <- glmMIC(formula=y~.-1, preselection=~age+sex, family="gaussian", a0=20, data=dat)
  fit.MIC

  # LOGISTIC REGRESSION
  library(ncvreg); data(heart)
  dat <- as.data.frame(cbind(scale(heart[, -10]), chd=heart$chd)); names(dat)
  dat <- dat[, -c(4, 6)]; head(dat)
  fit.MIC <- glmMIC(formula= chd~., data=dat, family = "binomial")
  fit.MIC

 # LOGLINEAR REGRESSION
 fish <- read.csv("http://www.ats.ucla.edu/stat/data/fish.csv")
 form <- count ~ . -1 + xb:zg
 y <- fish[,names(fish)==as.character(form)[2]]
 X <- model.matrix(as.formula(form),fish)
 dat <- data.frame(scale(X), count=fish$count); head(dat)
 fit.MIC <- glmMIC(formula= count~ ., family = "poisson", data=dat)
 fit.MIC

}
\references{
\itemize{
\item Su, X. (2015). Variable selection via subtle uprooting.
\emph{Journal of Computational and Graphical Statistics}, \bold{24}(4): 1092--1113.
URL \url{http://www.tandfonline.com/doi/pdf/10.1080/10618600.2014.955176}
\item Su, X., Fan, J., Levine, R. A., Nunn, M. E., and Tsai, C.-L. (2016+). Sparse estimation of generalized linear
models via approximated information criteria. Submitted, \emph{Statistica Sinica}.
}
}
\seealso{
\code{\link[stats]{glm}}, \code{\link[glmMIC]{print.glmMIC}}, \code{\link[glmMIC]{plot.glmMIC}},
}

